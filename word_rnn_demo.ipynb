{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"war_text.pickle\", \"rb\") as file:\n",
    "    war_text = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = ''.join(war_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Кримската', ' ', 'война', ' ', '(', 'октомври', ' ', '1', '8', '5']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bul_symbols = \"АБВГДЕЖЗИЙКЛМНОПРСТУФХЦЧШЩЬЪЮЯабвгдежзийклмнопрстуфхцчшщьъюя\"\n",
    "\n",
    "def split_text(txt):\n",
    "    ans = []\n",
    "    curr = ''\n",
    "    for c in txt:\n",
    "        if c in bul_symbols:\n",
    "            curr += c\n",
    "        else:\n",
    "            if curr!='': ans.append(curr)\n",
    "            ans.append(c)\n",
    "            curr = ''\n",
    "    if curr!='': ans.append(curr)\n",
    "    return ans\n",
    "\n",
    "splited_text = split_text(all_text)\n",
    "splited_text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38912"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word = sorted(set(splited_text))\n",
    "word2id = {w:i for i,w in enumerate(id2word)}\n",
    "vocab_size = len(id2word)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38912, 128)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_size = 8\n",
    "embedding_dim = 128\n",
    "\n",
    "cbow_model = tf.keras.Sequential()\n",
    "cbow_model.add(tf.keras.layers.Embedding(vocab_size, embedding_dim, input_shape = (window_size-1,)))\n",
    "cbow_model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "cbow_model.add(tf.keras.layers.Dropout(0.4))\n",
    "cbow_model.add(tf.keras.layers.Dense(vocab_size))\n",
    "\n",
    "cbow_model.load_weights('./cbow\\\\ckpt_10')\n",
    "\n",
    "cbow_model.get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 128)            4980736   \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (1, None, 1024)           3545088   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 128)            443136    \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (1, None, 128)            99072     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 38912)          5019648   \n",
      "=================================================================\n",
      "Total params: 14,087,680\n",
      "Trainable params: 9,106,944\n",
      "Non-trainable params: 4,980,736\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_units = 1024\n",
    "rnn_units2 = 128\n",
    "\n",
    "def build_model(batch_size=64):\n",
    "    embed = tf.keras.layers.Embedding(vocab_size, \n",
    "                                      embedding_dim, \n",
    "                                      batch_input_shape = [batch_size, None], \n",
    "                                      embeddings_initializer = tf.constant_initializer(cbow_model.get_weights()[0]), \n",
    "                                      trainable=False)\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(embed)\n",
    "    model.add(tf.keras.layers.GRU(rnn_units, \n",
    "                                  return_sequences=True, \n",
    "                                  stateful=True, \n",
    "                                  recurrent_initializer='glorot_uniform'))\n",
    "    model.add(tf.keras.layers.GRU(rnn_units2, \n",
    "                                  return_sequences=True, \n",
    "                                  stateful=True, \n",
    "                                  recurrent_initializer='glorot_uniform'))\n",
    "    model.add(tf.keras.layers.GRU(rnn_units2, \n",
    "                                  return_sequences=True, \n",
    "                                  stateful=True, \n",
    "                                  recurrent_initializer='glorot_uniform'))\n",
    "    model.add(tf.keras.layers.Dense(vocab_size))\n",
    "    return model\n",
    "\n",
    "model = build_model(1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(tf.train.latest_checkpoint('./word_rnn_embed'))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, num_generate = 200, temperature = 1.0):\n",
    "\n",
    "    input_eval = [word2id[s] for s in split_text(start_string)]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    text_generated = []\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(id2word[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Адолф Хитлер е убит на Византийската република е въоръжен втората в порядъка на България в Тристранния пакт войни. На 1 май 19 май 19 ноември 19 май 19 ноември 19 декември 19 ноември 19 ноември 19 септември 19 май 1919 г. до 19 май 1915 г. в битката при Сталинград (19 януари 1711 г. до 19 септември 19 октомври – май май 1946 г. до 16 април – 19 декември 19 януари 1919 г. в двореца на Третия райх България (19 януари 18\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, \"Адолф Хитлер е \", 200, 0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
